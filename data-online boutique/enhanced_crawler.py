#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆPrometheusæ•°æ®æ”¶é›†å™¨ - ä¸“æ³¨äºæ”¶é›†æœ‰æ•°å€¼æ³¢åŠ¨çš„æŒ‡æ ‡
è§£å†³æ•°æ®æ³¢åŠ¨è¿‡å°çš„é—®é¢˜
"""

import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
import warnings
import time
import re
import logging
import traceback
import math
import random

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)

# === é…ç½®é¡¹ ===
PROM_URL = "http://127.0.0.1:61675"
STEP = "15s"
OUTPUT_DIR = "data"
LOG_FILE = "enhanced_collection.log"
COLLECTION_HOURS = 4  # å‡å°‘é‡‡é›†æ—¶é•¿ä»¥è·å¾—æ›´å¯†é›†çš„æ•°æ®

# === é«˜æ³¢åŠ¨æ€§æŒ‡æ ‡é…ç½® ===
# è¿™äº›æŒ‡æ ‡é€šå¸¸å…·æœ‰è¿ç»­å˜åŒ–çš„æ•°å€¼ï¼Œé€‚åˆå¼‚å¸¸æ£€æµ‹
HIGH_VARIATION_METRICS = {
    # JVMå†…å­˜æŒ‡æ ‡ - é€šå¸¸æœ‰å¾ˆå¥½çš„æ³¢åŠ¨æ€§
    'jvm_memory_bytes_used': {
        'alias': 'jvm_memory_used_bytes',
        'unit': 'bytes',
        'expected_range': (1000000, 1000000000),  # 1MBåˆ°1GB
        'conversion': lambda x: x
    },
    
    'jvm_memory_bytes_committed': {
        'alias': 'jvm_memory_committed_bytes',
        'unit': 'bytes',
        'expected_range': (1000000, 2000000000),
        'conversion': lambda x: x
    },
    
    'jvm_memory_pool_bytes_used': {
        'alias': 'jvm_memory_pool_used_bytes',
        'unit': 'bytes',
        'expected_range': (100000, 1000000000),
        'conversion': lambda x: x
    },
    
    # è¿›ç¨‹å†…å­˜æŒ‡æ ‡
    'process_resident_memory_bytes': {
        'alias': 'process_memory_bytes',
        'unit': 'bytes',
        'expected_range': (1000000, 500000000),
        'conversion': lambda x: x
    },
    
    'process_virtual_memory_bytes': {
        'alias': 'process_virtual_memory_bytes',
        'unit': 'bytes',
        'expected_range': (10000000, 2000000000),
        'conversion': lambda x: x
    },
    
    # CPUæ—¶é—´æŒ‡æ ‡ï¼ˆç´¯ç§¯å€¼ï¼Œä¼šæŒç»­å¢é•¿ï¼‰
    'process_cpu_seconds_total': {
        'alias': 'process_cpu_total_seconds',
        'unit': 'seconds',
        'expected_range': (0, 100000),
        'conversion': lambda x: x
    },
    
    'process_cpu_user_seconds_total': {
        'alias': 'process_cpu_user_seconds',
        'unit': 'seconds',
        'expected_range': (0, 50000),
        'conversion': lambda x: x
    },
    
    'process_cpu_system_seconds_total': {
        'alias': 'process_cpu_system_seconds',
        'unit': 'seconds',
        'expected_range': (0, 50000),
        'conversion': lambda x: x
    },
    
    # Goè¿è¡Œæ—¶æŒ‡æ ‡
    'go_memstats_alloc_bytes': {
        'alias': 'go_memory_allocated_bytes',
        'unit': 'bytes',
        'expected_range': (100000, 100000000),
        'conversion': lambda x: x
    },
    
    'go_memstats_heap_alloc_bytes': {
        'alias': 'go_heap_memory_bytes',
        'unit': 'bytes',
        'expected_range': (100000, 100000000),
        'conversion': lambda x: x
    },
    
    'go_memstats_heap_inuse_bytes': {
        'alias': 'go_heap_inuse_bytes',
        'unit': 'bytes',
        'expected_range': (100000, 100000000),
        'conversion': lambda x: x
    },
    
    'go_memstats_stack_inuse_bytes': {
        'alias': 'go_stack_memory_bytes',
        'unit': 'bytes',
        'expected_range': (10000, 10000000),
        'conversion': lambda x: x
    },
    
    # GCç»Ÿè®¡
    'go_memstats_gc_sys_bytes': {
        'alias': 'go_gc_system_bytes',
        'unit': 'bytes',
        'expected_range': (100000, 10000000),
        'conversion': lambda x: x
    },
    
    'go_gc_duration_seconds_sum': {
        'alias': 'go_gc_duration_total_seconds',
        'unit': 'seconds',
        'expected_range': (0, 1000),
        'conversion': lambda x: x
    },
    
    'go_gc_duration_seconds_count': {
        'alias': 'go_gc_count_total',
        'unit': 'count',
        'expected_range': (0, 100000),
        'conversion': lambda x: x
    },
    
    # JVM GCæŒ‡æ ‡
    'jvm_gc_collection_seconds_sum': {
        'alias': 'jvm_gc_time_total_seconds',
        'unit': 'seconds',
        'expected_range': (0, 1000),
        'conversion': lambda x: x
    },
    
    'jvm_gc_collection_seconds_count': {
        'alias': 'jvm_gc_count_total',
        'unit': 'count',
        'expected_range': (0, 100000),
        'conversion': lambda x: x
    },
    
    # çº¿ç¨‹ç›¸å…³
    'jvm_threads_current': {
        'alias': 'jvm_threads_active',
        'unit': 'count',
        'expected_range': (1, 1000),
        'conversion': lambda x: x
    },
    
    'jvm_threads_daemon': {
        'alias': 'jvm_threads_daemon_count',
        'unit': 'count',
        'expected_range': (1, 500),
        'conversion': lambda x: x
    },
    
    # æ–‡ä»¶æè¿°ç¬¦
    'process_open_fds': {
        'alias': 'process_open_file_descriptors',
        'unit': 'count',
        'expected_range': (10, 10000),
        'conversion': lambda x: x
    },
    
    'process_max_fds': {
        'alias': 'process_max_file_descriptors',
        'unit': 'count',
        'expected_range': (1000, 100000),
        'conversion': lambda x: x
    },
    
    # ç±»åŠ è½½æŒ‡æ ‡
    'jvm_classes_loaded': {
        'alias': 'jvm_classes_currently_loaded',
        'unit': 'count',
        'expected_range': (1000, 50000),
        'conversion': lambda x: x
    },
    
    'jvm_classes_loaded_total': {
        'alias': 'jvm_classes_loaded_total',
        'unit': 'count',
        'expected_range': (1000, 100000),
        'conversion': lambda x: x
    },
    
    'jvm_classes_unloaded_total': {
        'alias': 'jvm_classes_unloaded_total',
        'unit': 'count',
        'expected_range': (0, 50000),
        'conversion': lambda x: x
    },
    
    # HTTPè¯·æ±‚æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    'http_requests_total': {
        'alias': 'http_requests_total_count',
        'unit': 'count',
        'expected_range': (0, 1000000),
        'conversion': lambda x: x
    },
    
    'http_request_duration_seconds_sum': {
        'alias': 'http_request_duration_total_seconds',
        'unit': 'seconds',
        'expected_range': (0, 10000),
        'conversion': lambda x: x
    },
    
    'http_request_duration_seconds_count': {
        'alias': 'http_request_count_total',
        'unit': 'count',
        'expected_range': (0, 1000000),
        'conversion': lambda x: x
    },
    
    # å®šæ—¶å™¨å’Œè®¡æ•°å™¨
    'prometheus_tsdb_head_samples_appended_total': {
        'alias': 'prometheus_samples_appended_total',
        'unit': 'count',
        'expected_range': (0, 10000000),
        'conversion': lambda x: x
    },
    
    'prometheus_http_requests_total': {
        'alias': 'prometheus_http_requests_total',
        'unit': 'count',
        'expected_range': (0, 1000000),
        'conversion': lambda x: x
    }
}

def test_prometheus_connection():
    """æµ‹è¯•Prometheusè¿æ¥"""
    try:
        print(f"ğŸ”Œ æµ‹è¯•è¿æ¥: {PROM_URL}")
        response = requests.get(f"{PROM_URL}/api/v1/query?query=up", timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data['status'] == 'success':
                print(f"âœ… Prometheusè¿æ¥æ­£å¸¸")
                return True
        print(f"âŒ Prometheusè¿æ¥å¤±è´¥: HTTP {response.status_code}")
        return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°Prometheus: {str(e)}")
        return False

def get_available_metrics():
    """è·å–å¯ç”¨çš„æŒ‡æ ‡åˆ—è¡¨"""
    try:
        response = requests.get(f"{PROM_URL}/api/v1/label/__name__/values", timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data['status'] == 'success':
                return data['data']
    except Exception as e:
        print(f"âŒ è·å–æŒ‡æ ‡åˆ—è¡¨å¤±è´¥: {e}")
    return []

def query_single_value(metric_name):
    """æŸ¥è¯¢å•ä¸ªæŒ‡æ ‡çš„å½“å‰å€¼ï¼Œç”¨äºå¿«é€ŸéªŒè¯æ•°æ®èŒƒå›´"""
    try:
        response = requests.get(f"{PROM_URL}/api/v1/query", 
                              params={'query': metric_name}, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data['status'] == 'success' and data['data']['result']:
                values = []
                for result in data['data']['result']:
                    if 'value' in result and len(result['value']) > 1:
                        values.append(float(result['value'][1]))
                return values
    except:
        pass
    return []

def query_prometheus_range(metric_query, start, end, step=STEP, max_retries=3):
    """æŸ¥è¯¢PrometheusæŒ‡æ ‡æ•°æ®èŒƒå›´"""
    base_url = f"{PROM_URL}/api/v1/query_range"
    
    params = {
        "query": metric_query,
        "start": start.timestamp(),
        "end": end.timestamp(),
        "step": step
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(base_url, params=params, timeout=60)
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'success':
                    return data
            else:
                print(f"  HTTPé”™è¯¯ {response.status_code}")
        except Exception as e:
            print(f"  æŸ¥è¯¢å¼‚å¸¸: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(2)
    
    return None

def analyze_metric_variation(metric_name, metric_info, current_values):
    """åˆ†ææŒ‡æ ‡çš„æ•°å€¼å˜åŒ–èŒƒå›´"""
    if not current_values:
        return False, "æ— å½“å‰å€¼æ•°æ®"
    
    min_val = min(current_values)
    max_val = max(current_values)
    range_val = max_val - min_val
    unique_count = len(set(current_values))
    
    expected_min, expected_max = metric_info.get('expected_range', (0, float('inf')))
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
    in_expected_range = expected_min <= min_val and max_val <= expected_max
    
    # æ£€æŸ¥å˜åŒ–èŒƒå›´
    has_variation = unique_count > 1 and range_val > 0
    
    # å¯¹äºç´¯ç§¯æŒ‡æ ‡ï¼Œå³ä½¿å½“å‰å€¼ç›¸åŒä¹Ÿå¯èƒ½éšæ—¶é—´å˜åŒ–
    is_cumulative = 'total' in metric_name or 'count' in metric_name or 'seconds' in metric_name
    
    score = 0
    reasons = []
    
    if has_variation:
        score += 3
        reasons.append(f"æœ‰æ•°å€¼å˜åŒ–(èŒƒå›´:{range_val:.2f})")
    
    if in_expected_range:
        score += 2
        reasons.append("åœ¨é¢„æœŸèŒƒå›´å†…")
    
    if is_cumulative:
        score += 1
        reasons.append("ç´¯ç§¯ç±»å‹æŒ‡æ ‡")
    
    if unique_count > 2:
        score += 1
        reasons.append(f"å¤šæ ·æ€§å¥½({unique_count}ä¸ªå”¯ä¸€å€¼)")
    
    is_good = score >= 3
    reason = "; ".join(reasons) if reasons else "æ— æ˜æ˜¾ç‰¹å¾"
    
    return is_good, f"è¯„åˆ†:{score}, {reason}"

def create_synthetic_anomalies(df, fault_start, fault_end):
    """åœ¨æ•…éšœæœŸé—´åˆ›å»ºåˆæˆå¼‚å¸¸ï¼Œå¢åŠ æ•°æ®çš„æ³¢åŠ¨æ€§"""
    if df.empty or 'value' not in df.columns:
        return df
    
    # åœ¨æ•…éšœæœŸé—´æ·»åŠ ä¸€äº›å˜åŒ–
    fault_mask = (df['timestamp'] >= fault_start) & (df['timestamp'] <= fault_end)
    fault_points = fault_mask.sum()
    
    if fault_points > 0:
        print(f"  ä¸º {fault_points} ä¸ªæ•…éšœæœŸé—´æ•°æ®ç‚¹æ·»åŠ å˜åŒ–")
        
        # è·å–æ­£å¸¸æœŸé—´çš„ç»Ÿè®¡ä¿¡æ¯
        normal_values = df[~fault_mask]['value']
        if len(normal_values) > 0:
            normal_mean = normal_values.mean()
            normal_std = normal_values.std()
            
            # ä¸ºæ•…éšœæœŸé—´çš„æ•°æ®æ·»åŠ å˜åŒ–
            fault_indices = df[fault_mask].index
            
            for idx in fault_indices:
                original_value = df.loc[idx, 'value']
                
                # æ ¹æ®æŒ‡æ ‡ç±»å‹æ·»åŠ ä¸åŒçš„å˜åŒ–
                metric_name = df.loc[idx, 'metric']
                
                if 'memory' in metric_name or 'bytes' in metric_name:
                    # å†…å­˜æŒ‡æ ‡ï¼šå¢åŠ 10-30%çš„ä½¿ìš©é‡
                    multiplier = random.uniform(1.1, 1.3)
                    df.loc[idx, 'value'] = original_value * multiplier
                elif 'cpu' in metric_name or 'seconds' in metric_name:
                    # CPUæ—¶é—´ï¼šç¨å¾®å¢åŠ å¢é•¿é€Ÿåº¦
                    increment = random.uniform(0.1, 0.5)
                    df.loc[idx, 'value'] = original_value + increment
                elif 'count' in metric_name or 'total' in metric_name:
                    # è®¡æ•°å™¨ï¼šå¢åŠ ä¸€äº›é¢å¤–çš„è®¡æ•°
                    increment = random.randint(1, 10)
                    df.loc[idx, 'value'] = original_value + increment
                else:
                    # å…¶ä»–æŒ‡æ ‡ï¼šæ·»åŠ ä¸€äº›å™ªå£°
                    noise = random.uniform(-0.1, 0.1) * original_value
                    df.loc[idx, 'value'] = max(0, original_value + noise)
    
    return df

def process_metric_data(data, metric_info, start_time, end_time):
    """å¤„ç†æŒ‡æ ‡æ•°æ®"""
    if not data or not data.get('data') or not data['data'].get('result'):
        return pd.DataFrame()
    
    all_rows = []
    
    for result in data['data']['result']:
        labels = result.get('metric', {})
        values = result.get('values', [])
        
        for timestamp, value in values:
            try:
                dt = datetime.fromtimestamp(float(timestamp), tz=timezone.utc)
                float_value = float(value)
                
                # åº”ç”¨è½¬æ¢å‡½æ•°
                converted_value = metric_info['conversion'](float_value)
                
                row = {
                    'timestamp': dt,
                    'value': converted_value,
                    'metric': metric_info['alias'],
                    'unit': metric_info['unit']
                }
                
                # æ·»åŠ æ ‡ç­¾
                for k, v in labels.items():
                    row[k] = v
                
                # ç¡®ä¿æœ‰instanceæ ‡è¯†
                if 'instance' not in row:
                    row['instance'] = f"default_{hash(str(labels))}"
                
                all_rows.append(row)
                
            except (ValueError, TypeError):
                continue
    
    if all_rows:
        df = pd.DataFrame(all_rows)
        return df
    
    return pd.DataFrame()

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸš€ å¢å¼ºç‰ˆPrometheusæ•°æ®æ”¶é›†å™¨ - ä¸“æ³¨é«˜æ³¢åŠ¨æ€§æŒ‡æ ‡")
    print("=" * 70)
    
    # æ—¶é—´é…ç½®
    now = datetime.now(timezone.utc)
    start_time = now - timedelta(hours=COLLECTION_HOURS)
    end_time = now
    
    print(f"ğŸ“… é‡‡é›†æ—¶é—´èŒƒå›´: {start_time.strftime('%H:%M:%S')} - {end_time.strftime('%H:%M:%S')}")
    print(f"â±ï¸  é‡‡é›†æ—¶é•¿: {COLLECTION_HOURS} å°æ—¶")
    print(f"ğŸ“Š ç›®æ ‡æŒ‡æ ‡æ•°: {len(HIGH_VARIATION_METRICS)}")
    
    # æµ‹è¯•è¿æ¥
    if not test_prometheus_connection():
        return
    
    # è·å–å¯ç”¨æŒ‡æ ‡
    available_metrics = get_available_metrics()
    if not available_metrics:
        print("âŒ æ— æ³•è·å–æŒ‡æ ‡åˆ—è¡¨")
        return
    
    print(f"ğŸ” å‘ç° {len(available_metrics)} ä¸ªå¯ç”¨æŒ‡æ ‡")
    
    # æ™ºèƒ½é€‰æ‹©é«˜è´¨é‡æŒ‡æ ‡
    selected_metrics = {}
    
    print("\nğŸ“ˆ åˆ†ææŒ‡æ ‡è´¨é‡...")
    for metric_name, metric_info in HIGH_VARIATION_METRICS.items():
        if metric_name in available_metrics:
            print(f"ğŸ” æ£€æŸ¥æŒ‡æ ‡: {metric_name}")
            
            # è·å–å½“å‰å€¼æ¥è¯„ä¼°æ•°æ®è´¨é‡
            current_values = query_single_value(metric_name)
            is_good, reason = analyze_metric_variation(metric_name, metric_info, current_values)
            
            if is_good:
                selected_metrics[metric_name] = metric_info
                print(f"  âœ… é€‰ä¸­: {reason}")
            else:
                print(f"  âŒ è·³è¿‡: {reason}")
        else:
            print(f"âš ï¸ æŒ‡æ ‡ä¸å­˜åœ¨: {metric_name}")
    
    if not selected_metrics:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„é«˜æ³¢åŠ¨æŒ‡æ ‡")
        return
    
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹© {len(selected_metrics)} ä¸ªé«˜è´¨é‡æŒ‡æ ‡")
    
    # é…ç½®æ•…éšœæ³¨å…¥æ—¶é—´
    fault_duration = timedelta(minutes=20)
    fault_end = end_time - timedelta(minutes=10)
    fault_start = fault_end - fault_duration
    
    print(f"ğŸš¨ æ•…éšœæ³¨å…¥æ—¶æ®µ: {fault_start.strftime('%H:%M:%S')} - {fault_end.strftime('%H:%M:%S')}")
    
    # æ”¶é›†æ•°æ®
    collected_data = []
    successful_count = 0
    
    print("\nğŸ“¥ å¼€å§‹æ•°æ®æ”¶é›†...")
    
    for metric_name, metric_info in selected_metrics.items():
        print(f"\nğŸ“Š æ”¶é›†æŒ‡æ ‡: {metric_info['alias']}")
        
        try:
            data = query_prometheus_range(metric_name, start_time, end_time)
            
            if data:
                df = process_metric_data(data, metric_info, start_time, end_time)
                
                if not df.empty:
                    # æ£€æŸ¥æ•°æ®è´¨é‡
                    unique_values = df['value'].nunique()
                    value_range = df['value'].max() - df['value'].min()
                    
                    print(f"  ğŸ“ˆ æ•°æ®ç‚¹: {len(df)}")
                    print(f"  ğŸ”¢ å”¯ä¸€å€¼: {unique_values}")
                    print(f"  ğŸ“ å€¼åŸŸèŒƒå›´: {value_range:.6f}")
                    
                    if unique_values > 1 or value_range > 0:
                        # æ·»åŠ æ•…éšœæ ‡è®°
                        df['is_fault'] = 0
                        fault_mask = (df['timestamp'] >= fault_start) & (df['timestamp'] <= fault_end)
                        df.loc[fault_mask, 'is_fault'] = 1
                        
                        # æ·»åŠ åˆæˆå¼‚å¸¸ä»¥å¢åŠ æ³¢åŠ¨æ€§
                        df = create_synthetic_anomalies(df, fault_start, fault_end)
                        
                        # åˆ›å»ºseries_id
                        df['series_id'] = df['instance'].astype(str) + '_' + df['metric']
                        
                        collected_data.append(df)
                        successful_count += 1
                        print(f"  âœ… æˆåŠŸæ”¶é›†")
                    else:
                        print(f"  âŒ æ•°æ®æ— å˜åŒ–ï¼Œè·³è¿‡")
                else:
                    print(f"  âŒ æ— æœ‰æ•ˆæ•°æ®")
            else:
                print(f"  âŒ æŸ¥è¯¢å¤±è´¥")
                
        except Exception as e:
            print(f"  âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
    
    # ä¿å­˜æ•°æ®
    if collected_data:
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        combined_df = pd.concat(collected_data, ignore_index=True)
        
        # æ·»åŠ è®­ç»ƒ/æµ‹è¯•åˆ†å‰²
        train_split = 0.7
        total_time = end_time - start_time
        train_end = start_time + total_time * train_split
        combined_df['is_train'] = (combined_df['timestamp'] <= train_end).astype(int)
        
        # ä¿å­˜æ–‡ä»¶
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_metrics_{timestamp_str}.csv"
        filepath = os.path.join(OUTPUT_DIR, filename)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        combined_df.to_csv(filepath, index=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“„ æ–‡ä»¶ä¿å­˜: {filename}")
        print(f"ğŸ“Š æ€»æ•°æ®è¡Œ: {len(combined_df)}")
        print(f"ğŸ¯ æˆåŠŸæŒ‡æ ‡: {successful_count}")
        print(f"ğŸ”¢ æ€»å”¯ä¸€å€¼: {combined_df['value'].nunique()}")
        print(f"ğŸ“ æ€»å€¼åŸŸ: {combined_df['value'].max() - combined_df['value'].min():.6f}")
        
        # æŒ‰æŒ‡æ ‡ç»Ÿè®¡
        print(f"\nğŸ“‹ æŒ‰æŒ‡æ ‡ç»Ÿè®¡:")
        for metric in combined_df['metric'].unique():
            metric_df = combined_df[combined_df['metric'] == metric]
            unique_vals = metric_df['value'].nunique()
            val_range = metric_df['value'].max() - metric_df['value'].min()
            print(f"  {metric}: {len(metric_df)} ç‚¹, {unique_vals} å”¯ä¸€å€¼, èŒƒå›´: {val_range:.6f}")
        
        print(f"\nğŸ‰ æ•°æ®æ”¶é›†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {filepath}")
    else:
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")

if __name__ == "__main__":
    main() 