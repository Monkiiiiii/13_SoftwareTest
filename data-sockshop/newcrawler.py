import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
import warnings
import time
import re
import logging
import traceback
import math

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)

# === é…ç½®é¡¹ ===
PROM_URL = "http://127.0.0.1:51384"  # Prometheusç«¯å£
STEP = "15s"  # æ•°æ®é‡‡é›†æ­¥é•¿
OUTPUT_DIR = "data"  # è¾“å‡ºç›®å½•
LOG_FILE = "data_collection.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„

# === æ ¸å¿ƒç›‘æŽ§æŒ‡æ ‡ ===
METRICS = {
    # ä½¿ç”¨container_cpu_usage_seconds_totalåŽŸå§‹æ•°æ®
    'container_cpu_usage_seconds_total': {
        'alias': 'cpu_usage_seconds',
        'unit': 'seconds',
        'normal_max': None,
        'conversion': lambda x: x
    },
    
    # å†…å­˜ä½¿ç”¨ï¼ˆå­—èŠ‚è½¬æ¢ä¸ºMBï¼‰
    'container_memory_usage_bytes': {
        'alias': 'mem_usage',
        'unit': 'MB',
        'normal_max': None,
        'conversion': lambda x: x / (1024 * 1024)
    },
    
    # è¿è¡Œçš„è¿›ç¨‹æ•°
    'processes_total': {
        'alias': 'process_count',
        'unit': 'count',
        'normal_max': None,
        'conversion': lambda x: x
    }
}

# === è¾…åŠ©å‡½æ•° ===
def test_prometheus_connection():
    """æµ‹è¯•Prometheusè¿žæŽ¥"""
    try:
        print(f"ðŸ”Œ æµ‹è¯•è¿žæŽ¥: {PROM_URL}")
        start_time = time.time()
        response = requests.get(f"{PROM_URL}/api/v1/query?query=up", timeout=10)
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            if data['status'] == 'success':
                print(f"âœ… Prometheusè¿žæŽ¥æ­£å¸¸ (å“åº”æ—¶é—´: {elapsed:.2f}s)")
                # æ·»åŠ å¥åº·æ£€æŸ¥
                result = data.get('data', {}).get('result', [])
                if result:
                    print(f"  æ£€æµ‹åˆ° {len(result)} ä¸ªæ´»è·ƒæœåŠ¡")
                else:
                    print("âš ï¸ æ— æ´»è·ƒæœåŠ¡æŠ¥å‘Š - é…ç½®å¯èƒ½å­˜åœ¨é—®é¢˜")
                return True
            
        print(f"âŒ Prometheusè¿žæŽ¥å¤±è´¥: HTTP {response.status_code}")
        # å°è¯•èŽ·å–ç‰ˆæœ¬ä¿¡æ¯ä½œä¸ºå¤‡é€‰
        try:
            version_res = requests.get(f"{PROM_URL}/api/v1/status/buildinfo", timeout=5)
            if version_res.status_code == 200:
                version = version_res.json().get('data', {}).get('version', 'æœªçŸ¥')
                print(f"  æ£€æµ‹åˆ°Prometheusç‰ˆæœ¬: {version}")
        except:
            pass
        return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿žæŽ¥åˆ°Prometheus: {str(e)}")
        return False

def query_prometheus(metric_query, start, end, step=STEP, max_retries=5):
    """æŸ¥è¯¢PrometheusæŒ‡æ ‡æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰- ä¿®å¤ç‰ˆæœ¬"""
    base_url = f"{PROM_URL}/api/v1/query_range"
    
    # å¦‚æžœæŸ¥è¯¢åŒ…å«CPUæŒ‡æ ‡ï¼Œåˆ‡æ¢åˆ°åŽŸå§‹æŒ‡æ ‡æŸ¥è¯¢
    if "cpu_usage_seconds_total" in metric_query:
        # ä½¿ç”¨åŽŸå§‹æ•°æ®æŸ¥è¯¢è€Œä¸æ˜¯èšåˆæŸ¥è¯¢
        metric_query = "container_cpu_usage_seconds_total"
        print("ðŸ”§ å·²ä¿®æ”¹CPUæŸ¥è¯¢ä¸ºåŽŸå§‹æŒ‡æ ‡: container_cpu_usage_seconds_total")
    
    params = {
        "query": metric_query,
        "start": start.timestamp(),
        "end": end.timestamp(),
        "step": step
    }
    
    print(f"  æŸ¥è¯¢è¯­å¥: {metric_query}")
    duration = (end - start).total_seconds() / 3600
    print(f"  æ—¶é—´èŒƒå›´: {start.strftime('%Y-%m-%d %H:%M:%S')} â†’ {end.strftime('%Y-%m-%d %H:%M:%S')} ({duration:.2f}å°æ—¶)")
    print(f"  è¯·æ±‚æ­¥é•¿: {step}")
    
    for attempt in range(max_retries):
        try:
            response = requests.get(base_url, params=params, timeout=60)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code != 200:
                error_info = f"HTTP {response.status_code} é”™è¯¯"
                try:
                    error_data = response.json()
                    error_info += f": {error_data.get('errorType', 'æœªçŸ¥ç±»åž‹')} - {error_data.get('error', 'æ— é”™è¯¯è¯¦æƒ…')}"
                except:
                    error_info += f": {response.text[:200]}"
                print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {error_info}")
                return None
                
            data = response.json()
            
            # æ£€æŸ¥Prometheusè¿”å›žçš„çŠ¶æ€
            if data.get('status') != 'success':
                error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')
                print(f"âš ï¸ Prometheusè¿”å›žé”™è¯¯: {error_msg}")
                return None
                
            # æ£€æŸ¥å®žé™…è¿”å›žçš„æ•°æ®ç‚¹æ•°é‡
            total_points = sum(len(series.get('values', [])) for series in data['data'].get('result', []))
            print(f"  è¿”å›žæ•°æ®ç‚¹æ•°é‡: {total_points}")
            
            # æ£€æŸ¥æ—¶é—´èŒƒå›´è¦†ç›–
            if data['data'].get('result'):
                values = data['data']['result'][0].get('values', [])
                if values:
                    first_ts = datetime.fromtimestamp(float(values[0][0]), tz=timezone.utc)
                    last_ts = datetime.fromtimestamp(float(values[-1][0]), tz=timezone.utc)
                    actual_duration = (last_ts - first_ts).total_seconds() / 3600
                    coverage = actual_duration / duration * 100 if duration > 0 else 100
                    print(f"  å®žé™…æ—¶é—´è¦†ç›–: {actual_duration:.2f}h/{duration:.2f}h ({coverage:.1f}%)")
            
            return data
            
        except (requests.RequestException, requests.ConnectionError) as e:
            wait_time = 2 ** attempt
            print(f"âš ï¸ ç½‘ç»œé”™è¯¯ï¼Œ{wait_time}ç§’åŽé‡è¯•: {str(e)}")
            time.sleep(wait_time)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢å¼‚å¸¸: {str(e)}")
            time.sleep(3)
    
    print(f"âŒ æŸ¥è¯¢å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})")
    return None

def normalize_label_name(name):
    """è§„èŒƒåŒ–æ ‡ç­¾åç§°"""
    try:
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[^a-zA-Z0-9_]', '_', str(name))
        # ç¡®ä¿ä¸ä»¥æ•°å­—å¼€å¤´
        if name[0].isdigit():
            name = f'label_{name}'
        return name.lower()
    except Exception as e:
        print(f"  æ ‡ç­¾åç§°è§„èŒƒåŒ–é”™è¯¯: {str(e)}")
        return f"error_{hash(str(e))}"

def parse_metric_data(data, metric_info, req_start, req_end):
    """è§£æžPrometheuså“åº”æ•°æ®"""
    if not data or not data.get('data') or not data['data'].get('result'):
        print(f"âš ï¸ æ— æœ‰æ•ˆæ•°æ®è¿”å›ž: {metric_info['alias']}")
        return pd.DataFrame(), req_start, req_end
    
    try:
        results = data['data']['result']
        total_points = sum(len(series.get('values', [])) for series in results)
        print(f"âœ… èŽ·å–åˆ° {len(results)} ä¸ªæ—¶é—´åºåˆ—ï¼Œå…± {total_points} ä¸ªæ•°æ®ç‚¹")
    except Exception as e:
        print(f"âŒ è§£æžç»“æžœå¤±è´¥: {str(e)}")
        return pd.DataFrame(), req_start, req_end
    
    all_dfs = []
    min_timestamp = req_end
    max_timestamp = req_start
    
    # è®°å½•æ¯ä¸ªåºåˆ—çš„æ•°æ®ç‚¹æ•°å’Œæ—¶é—´èŒƒå›´
    series_stats = []
    
    for result in results:
        labels = result.get('metric', {})
        values = result.get('values', [])
        
        # è¿½è¸ªå®žé™…æ—¶é—´èŒƒå›´
        if values:
            try:
                series_min = datetime.fromtimestamp(float(values[0][0]), tz=timezone.utc)
                series_max = datetime.fromtimestamp(float(values[-1][0]), tz=timezone.utc)
                min_timestamp = min(min_timestamp, series_min)
                max_timestamp = max(max_timestamp, series_max)
                
                # è®¡ç®—è¯¥ç³»åˆ—çš„æ—¶é—´è¦†ç›–çŽ‡å’Œç‚¹é—´éš”
                series_duration = (series_max - series_min).total_seconds()
                avg_interval = series_duration / (len(values) - 1) if len(values) > 1 else 0
                
                # å­˜å‚¨ç³»åˆ—ç»Ÿè®¡ä¿¡æ¯
                series_stats.append({
                    'series': list(labels.values())[0] if labels else "unknown",
                    'points': len(values),
                    'duration_min': series_duration / 60,
                    'avg_interval': avg_interval
                })
                
                # æ‰“å°æ¯ä¸ªç³»åˆ—çš„åŸºæœ¬ä¿¡æ¯
                label_str = ", ".join([f"{k}={v}" for k, v in labels.items()])
                print(f"  æ ‡ç­¾: {label_str} | æ•°æ®ç‚¹: {len(values)}ä¸ª | æŒç»­æ—¶é—´: {series_duration/60:.1f}åˆ†é’Ÿ | å¹³å‡é—´éš”: {avg_interval:.1f}ç§’")
                
            except Exception as e:
                print(f"  æ—¶é—´æˆ³è½¬æ¢é”™è¯¯: {str(e)}")
        
        points = []
        for point in values:
            try:
                timestamp, value = point
                dt = datetime.fromtimestamp(float(timestamp), tz=timezone.utc)
                float_value = metric_info['conversion'](float(value))
                
                row = {'timestamp': dt, 'value': float_value}
                for label_name, label_value in labels.items():
                    row[normalize_label_name(label_name)] = label_value
                
                points.append(row)
            except (ValueError, TypeError) as e:
                print(f"  æ•°æ®ç‚¹è§£æžé”™è¯¯: {str(e)}")
                continue
        
        if points:
            try:
                ts_df = pd.DataFrame(points)
                all_dfs.append(ts_df)
            except Exception as e:
                print(f"  åˆ›å»ºDataFrameå¤±è´¥: {str(e)}")

    if not all_dfs:
        return pd.DataFrame(), min_timestamp, max_timestamp
    
    try:
        combined_df = pd.concat(all_dfs, ignore_index=True)
        combined_df['metric'] = metric_info['alias']
        combined_df['unit'] = metric_info['unit']
    except Exception as e:
        print(f"  åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")
        return pd.DataFrame(), min_timestamp, max_timestamp
    
    # æ‰“å°è¯¦ç»†çš„ç³»åˆ—ç»Ÿè®¡ä¿¡æ¯
    if series_stats:
        total_points = sum(s['points'] for s in series_stats)
        avg_interval = sum(s['avg_interval'] for s in series_stats) / len(series_stats)
        min_duration = min(s['duration_min'] for s in series_stats)
        max_duration = max(s['duration_min'] for s in series_stats)
        
        print(f"  æ—¶é—´åºåˆ—ç»Ÿè®¡:")
        print(f"    æ€»æ•°æ®ç‚¹: {total_points}")
        print(f"    æœ€å°æŒç»­æ—¶é—´: {min_duration:.1f}åˆ†é’Ÿ | æœ€å¤§æŒç»­æ—¶é—´: {max_duration:.1f}åˆ†é’Ÿ")
        print(f"    å¹³å‡é—´éš”: {avg_interval:.1f}ç§’")
    
    # è¿”å›žå®žé™…æ—¶é—´èŒƒå›´
    return combined_df, min_timestamp, max_timestamp

def mark_fault_period(df, metric_info, fault_start, fault_end):
    """æ ‡è®°æ•…éšœæ³¨å…¥æœŸé—´çš„æ•°æ®ç‚¹"""
    if df.empty:
        return df
    
    # æ·»åŠ æ•…éšœæ ‡è®°
    df['is_fault'] = 0
    
    # ç¡®ä¿æœ‰æ—¶é—´åºåˆ—æ ‡è¯†
    if 'instance' not in df.columns:
        df['instance'] = 'default'
    
    # åˆ›å»ºå®žä¾‹+æŒ‡æ ‡çš„å¤åˆé”®
    if 'instance' in df.columns and 'metric' in df.columns:
        df['series_id'] = df['instance'].astype(str) + '_' + df['metric']
    else:
        print("âš ï¸ æ— æ³•åˆ›å»ºseries_idï¼Œç¼ºå°‘å¿…è¦çš„åˆ—")
        return df
    
    # æ ‡è®°æ•…éšœæœŸé—´çš„æ•°æ®ç‚¹
    fault_mask = (df['timestamp'] >= fault_start) & (df['timestamp'] <= fault_end)
    df.loc[fault_mask, 'is_fault'] = 1
    
    return df

def save_combined_data(all_data, fault_start, fault_end):
    """ä¿å­˜åˆå¹¶çš„æ•°æ®é›†"""
    if not all_data:
        print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
        return None, {}
    
    combined_df = pd.concat(all_data, ignore_index=True)
    
    if combined_df.empty:
        print("âš ï¸ ç©ºæ•°æ®é›†æ— æ³•ä¿å­˜")
        return None, {}
    
    # ç¡®ä¿æ‰€æœ‰æ—¶é—´åºåˆ—éƒ½æœ‰series_id
    if 'series_id' not in combined_df.columns:
        if 'instance' in combined_df.columns and 'metric' in combined_df.columns:
            combined_df['series_id'] = combined_df['instance'].astype(str) + '_' + combined_df['metric']
        else:
            print("âš ï¸ æ— æ³•åˆ›å»ºseries_id")
            return None, {}
    
    # ç§‘å­¦åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    earliest = combined_df['timestamp'].min()
    latest = combined_df['timestamp'].max()
    train_end = earliest + (latest - earliest) * 0.75
    combined_df['is_train'] = (combined_df['timestamp'] <= train_end).astype(int)
    
    # åˆ›å»ºæ–‡ä»¶å
    timestamp_str = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    filename = f"prometheus_metrics_{timestamp_str}.csv"
    filepath = os.path.join(OUTPUT_DIR, filename)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    # ä¿å­˜å¹¶æ·»åŠ æ—¶é—´èŒƒå›´ä¿¡æ¯
    time_range_note = f"å®žé™…æ•°æ®èŒƒå›´: {earliest.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {latest.strftime('%Y-%m-%d %H:%M:%S')}\n"
    time_range_note += f"è¯·æ±‚æ—¶é—´èŒƒå›´: {fault_start.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {fault_end.strftime('%Y-%m-%d %H:%M:%S')}\n"
    time_range_note += f"æ•…éšœæ³¨å…¥: {fault_start.strftime('%H:%M:%S')} - {fault_end.strftime('%H:%M:%S')}"
    
    with open(filepath.replace('.csv', '.note.txt'), 'w') as f:
        f.write(time_range_note)
    
    combined_df.to_csv(filepath, index=False)
    print(f"ðŸ’¾ å·²ä¿å­˜æ•°æ®é›†: {filepath} ({len(combined_df)}è¡Œ)")
    
    # è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®
    stats = {}
    total_samples = 0
    min_samples = float('inf')
    max_samples = 0
    
    print("\nðŸ“Š è¯¦ç»†æŒ‡æ ‡ç»Ÿè®¡:")
    for metric, info in METRICS.items():
        alias = info['alias']
        metric_df = combined_df[combined_df['metric'] == alias]
        if not metric_df.empty:
            # åˆ†ç»„ç»Ÿè®¡æ¯ä¸ªç³»åˆ—
            series_stats = metric_df.groupby('series_id').agg(
                points=('value', 'count'),
                min_value=('value', 'min'),
                max_value=('value', 'max'),
                start_time=('timestamp', 'min'),
                end_time=('timestamp', 'max')
            )
            
            series_stats['duration_h'] = (series_stats['end_time'] - series_stats['start_time']).dt.total_seconds() / 3600
            series_stats['avg_interval'] = series_stats['duration_h'] * 3600 / (series_stats['points'] - 1)
            
            min_duration = series_stats['duration_h'].min()
            max_duration = series_stats['duration_h'].max()
            avg_interval = series_stats['avg_interval'].mean()
            
            stats[alias] = {
                'samples': len(metric_df),
                'series': len(series_stats),
                'min_value': metric_df['value'].min(),
                'max_value': metric_df['value'].max(),
                'min_duration': min_duration,
                'max_duration': max_duration,
                'avg_interval': avg_interval
            }
            
            total_samples += len(metric_df)
            min_samples = min(min_samples, len(metric_df))
            max_samples = max(max_samples, len(metric_df))
            
            print(f"  {alias}:")
            print(f"    æ ·æœ¬æ•°: {len(metric_df)} | ç³»åˆ—æ•°: {len(series_stats)}")
            print(f"    å€¼èŒƒå›´: {metric_df['value'].min():.2f} - {metric_df['value'].max():.2f}")
            print(f"    æ—¶é—´è¦†ç›–: {min_duration:.2f}h - {max_duration:.2f}h")
            print(f"    å¹³å‡é‡‡æ ·é—´éš”: {avg_interval:.1f}ç§’")
    
    # æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
    overall_coverage = earliest.strftime('%H:%M:%S') + " - " + latest.strftime('%H:%M:%S')
    print(f"\nðŸ“Š æ•´ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  æœ€å°æ ·æœ¬æŒ‡æ ‡: {min_samples} | æœ€å¤§æ ·æœ¬æŒ‡æ ‡: {max_samples}")
    print(f"  æ—¶é—´èŒƒå›´è¦†ç›–: {overall_coverage}")
    
    return filepath, stats

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œæ•°æ®é‡‡é›†æµç¨‹"""
    try:
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šå½“å‰æ—¶é—´å‰8å°æ—¶åˆ°çŽ°åœ¨
        now = datetime.now(timezone.utc)
        print(f"\nðŸ•’ å½“å‰æ—¶é—´ (UTC): {now} ({now.timestamp()})")
        
        # æå‰å¼€å§‹æ—¶é—´ï¼ˆå¢žåŠ 5åˆ†é’Ÿç¼“å†²ï¼‰
        start_time = now - timedelta(hours=8, minutes=5)
        print(f"ðŸ•’ å¼€å§‹æ—¶é—´ (UTC): {start_time} ({start_time.timestamp()})")
        
        # ç»“æŸæ—¶é—´ä¸ºå½“å‰æ—¶é—´
        end_time = now
        
        print("\n" + "=" * 70)
        print("Prometheus æ•°æ®é‡‡é›†å·¥å…· (æœ€ç»ˆä¼˜åŒ–ç‰ˆ)")
        print("=" * 70)
        print(f"ç²¾ç¡®æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {end_time.strftime('%Y-%m-%d %H:%M:%S')} (UTC)")
        duration = (end_time - start_time).total_seconds() / 3600
        print(f"æ€»æ—¶é•¿: {duration:.2f} å°æ—¶")
        print(f"é‡‡é›†æ­¥é•¿: {STEP}")
        print(f"ç›‘æŽ§æŒ‡æ ‡æ•°: {len(METRICS)}")
        print(f"Prometheus URL: {PROM_URL}")
        print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
        # æµ‹è¯•è¿žæŽ¥
        if not test_prometheus_connection():
            print("è¯·æ£€æŸ¥Prometheus URLå’Œè¿žæŽ¥è®¾ç½®")
            return
        
        # é…ç½®æ•…éšœæ³¨å…¥æ—¶é—´æ®µ
        fault_duration = timedelta(minutes=15)
        fault_end = end_time - timedelta(minutes=5)
        fault_start = fault_end - fault_duration
        print(f"æ•…éšœæ³¨å…¥æ—¶æ®µ: {fault_start.strftime('%H:%M:%S')} - {fault_end.strftime('%H:%M:%S')}")
        
        collected_data = []
        print("\nå¼€å§‹é‡‡é›†æŒ‡æ ‡æ•°æ®...\n")
        
        for promql_query, metric_info in METRICS.items():
            print(f"\nðŸ” æ­£åœ¨é‡‡é›†: {metric_info['alias']}")
            print(f"  æŸ¥è¯¢æŒ‡æ ‡: {metric_info['alias']}")
            
            # ä½¿ç”¨åŽŸå§‹æŸ¥è¯¢
            data = query_prometheus(promql_query, start_time, end_time)
            
            # è§£æžæ•°æ®
            if data:
                df, min_ts, max_ts = parse_metric_data(data, metric_info, start_time, end_time)
                
                # æ£€æŸ¥æ•°æ®èŒƒå›´
                if not df.empty:
                    # æ·»åŠ æ•…éšœæ ‡è®°
                    df = mark_fault_period(df, metric_info, fault_start, fault_end)
                    collected_data.append(df)
                    print(f"âœ… {metric_info['alias']} é‡‡é›†å®Œæˆ: {len(df)} è¡Œæ•°æ®\n")
                else:
                    print(f"âŒ {metric_info['alias']} æ— æœ‰æ•ˆæ•°æ®\n")
            else:
                print(f"âŒ {metric_info['alias']} æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡æ­¤æŒ‡æ ‡\n")
        
        # ä¿å­˜æ•°æ®
        if collected_data:
            filepath, stats = save_combined_data(collected_data, fault_start, fault_end)
            print(f"\nðŸ’¾ æ•°æ®é›†å·²ä¿å­˜åˆ°: {filepath}")
        else:
            print("âš ï¸ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ•°æ®")
        
        print("\n" + "=" * 50)
        print("âœ… æ•°æ®é‡‡é›†å®Œæˆ!")
        print("=" * 50)
    
    except Exception as e:
        print(f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        
        # è°ƒè¯•å»ºè®®
        print("\nðŸ’¡ è°ƒè¯•å»ºè®®:")
        print(f"1. æ‰‹åŠ¨æµ‹è¯•æŸ¥è¯¢:")
        print(f"   curl -G '{PROM_URL}/api/v1/query_range'")
        print(f"     --data-urlencode 'query=container_cpu_usage_seconds_total'")
        print(f"     --data-urlencode 'start={start_time.timestamp()}'")
        print(f"     --data-urlencode 'end={end_time.timestamp()}'")
        print(f"     --data-urlencode 'step=15s'")
        print(f"2. æ£€æŸ¥Prometheusæ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        filename=os.path.join(OUTPUT_DIR, LOG_FILE),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # å¼€å§‹é‡‡é›†
    main()